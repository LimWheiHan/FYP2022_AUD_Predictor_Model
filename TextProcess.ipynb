{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fileinput\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import tokenize\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import uniform\n",
    "\n",
    "#import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "stopwords = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "w_tokenizer = tokenize.WhitespaceTokenizer()\n",
    "\n",
    "# Remove newline\n",
    "def remove_nline(value):\n",
    "    return ''.join(str(value).splitlines())\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text) if not w in set(stopwords)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4943 entries, 0 to 4942\n",
      "Data columns (total 13 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   subreddit             4943 non-null   object \n",
      " 1   title                 4943 non-null   object \n",
      " 2   selftext              3913 non-null   object \n",
      " 3   upvote_ratio          4943 non-null   float64\n",
      " 4   author                4943 non-null   object \n",
      " 5    author_flair_text    1411 non-null   object \n",
      " 6   ups                   4943 non-null   int64  \n",
      " 7   downs                 4943 non-null   int64  \n",
      " 8   score                 4943 non-null   int64  \n",
      " 9   kind                  4943 non-null   object \n",
      " 10  id                    4943 non-null   object \n",
      " 11  link_flair_css_class  1709 non-null   object \n",
      " 12  created_utc           4943 non-null   object \n",
      "dtypes: float64(1), int64(3), object(9)\n",
      "memory usage: 502.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('raw_new_posts_from_SD.xlsx')\n",
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14268\\2810795996.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testdf['selftext'] = testdf['title'] + \" \" +testdf['selftext']\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14268\\2810795996.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testdf['selftext'] = [remove_nline(i) for i in testdf['selftext']]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14268\\2810795996.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testdf[\"selftext\"]: list[str] = [str(x).translate(str.maketrans('', '', '\"#$%&\\()*+-/:;<=>@[\\\\]^_`{|}~')) for x in testdf[\"selftext\"]]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14268\\2810795996.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testdf['s_selftext'] = [tokenize.sent_tokenize(str(i), language=\"english\") for i in testdf['selftext']]\n"
     ]
    }
   ],
   "source": [
    "#Create Alcoholic Post & Label as 1\n",
    "\n",
    "df = data\n",
    "df[\" author_flair_text\"] = data[\" author_flair_text\"].replace(\"\\sdays||\\sday\",\"\",regex=True)\n",
    "\n",
    "#Convert badge to float dtype\n",
    "df = df.astype({\" author_flair_text\": 'float32'})\n",
    "\n",
    "#If get post from SD subreddit and has abstained for less than a year\n",
    "testdf= df[(df['subreddit'] == 'stopdrinking') & (df[' author_flair_text'] <= 365)]\n",
    "\n",
    "#Combine title and post\n",
    "testdf['selftext'] = testdf['title'] + \" \" +testdf['selftext']\n",
    "testdf['selftext'] = [remove_nline(i) for i in testdf['selftext']]\n",
    "\n",
    "# Remove most punctuations except !?,.' , these are needed for sentence spliting\n",
    "testdf[\"selftext\"]: list[str] = [str(x).translate(str.maketrans('', '', '\"#$%&\\()*+-/:;<=>@[\\\\]^_`{|}~')) for x in testdf[\"selftext\"]]\n",
    "\n",
    "#Break posts into list of sentences, cannot use \"post\" since it removes punctuation \n",
    "testdf['s_selftext'] = [tokenize.sent_tokenize(str(i), language=\"english\") for i in testdf['selftext']]\n",
    "\n",
    "# Create observations for each sentence\n",
    "newtestdf = testdf.explode('s_selftext') \n",
    "\n",
    "#Get subjectivity score for each sentence\n",
    "newtestdf['subjectivity'] = [TextBlob(str(i)).sentiment.subjectivity for i in newtestdf['s_selftext']]\n",
    "\n",
    "\n",
    "#new rows of labels split by sentences and calculate their subjectivity\n",
    "cols = ['subreddit', 's_selftext', 'subjectivity']\n",
    "target_df = newtestdf[cols]\n",
    "\n",
    "#Only use post where subjectivity is more than 0.1\n",
    "target_df= target_df[target_df['subjectivity'] >= 0.1]\n",
    "\n",
    "# Label alcolic post as 1\n",
    "target_df['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14268\\925545516.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ntestdf['selftext'] = ntestdf['title'] + \" \" +ntestdf['selftext']\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14268\\925545516.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ntestdf['selftext'] = [remove_nline(i) for i in ntestdf['selftext']]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14268\\925545516.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ntestdf[\"selftext\"]: list[str] = [str(x).translate(str.maketrans('', '', '\"#$%&\\()*+-/:;<=>@[\\\\]^_`{|}~')) for x in ntestdf[\"selftext\"]]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14268\\925545516.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ntestdf['s_selftext'] = [tokenize.sent_tokenize(str(i), language=\"english\") for i in ntestdf['selftext']]\n"
     ]
    }
   ],
   "source": [
    "#Create Non-Alcoholic Post & Label as 0\n",
    "\n",
    "\n",
    "df = data\n",
    "df[\" author_flair_text\"] = data[\" author_flair_text\"].replace(\"\\sdays||\\sday\",\"\",regex=True)\n",
    "\n",
    "#Convert badge to float dtype\n",
    "df = df.astype({\" author_flair_text\": 'float32'})\n",
    "\n",
    "#Create subset data for non target\n",
    "ntestdf= df[(df['subreddit'] != 'stopdrinking') | ((df[' author_flair_text'] >= 365) & (df['subreddit'] == 'stopdrinking') )]\n",
    "\n",
    " \n",
    "#Combine title and post\n",
    "ntestdf['selftext'] = ntestdf['title'] + \" \" +ntestdf['selftext']\n",
    "ntestdf['selftext'] = [remove_nline(i) for i in ntestdf['selftext']]\n",
    "\n",
    "# Remove most punctuations except !?,.', these are needed for sentence spliting\n",
    "ntestdf[\"selftext\"]: list[str] = [str(x).translate(str.maketrans('', '', '\"#$%&\\()*+-/:;<=>@[\\\\]^_`{|}~')) for x in ntestdf[\"selftext\"]]\n",
    "\n",
    "#Break posts into list of sentences, cannot use \"post\" since it removes punctuation \n",
    "ntestdf['s_selftext'] = [tokenize.sent_tokenize(str(i), language=\"english\") for i in ntestdf['selftext']]\n",
    "\n",
    "# Create observations for each sentence\n",
    "nnewtestdf = ntestdf.explode('s_selftext') \n",
    "\n",
    "#Get subjectivity score\n",
    "nnewtestdf['subjectivity'] = [TextBlob(str(i)).sentiment.subjectivity for i in nnewtestdf['s_selftext']]\n",
    "\n",
    "\n",
    "#new rows of labels split by sentences and calculate their subjectivity\n",
    "cols = ['subreddit', 's_selftext', 'subjectivity']\n",
    "ntarget_df = nnewtestdf[cols]\n",
    "\n",
    "#Only use post where subjectivity is more than 0.1\n",
    "ntarget_df= ntarget_df[ntarget_df['subjectivity'] >= 0.1]\n",
    "\n",
    "# Label alcolic post as 1\n",
    "ntarget_df['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining labels into input\n",
    "frames = [target_df,ntarget_df]\n",
    "\n",
    "input = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input['s_selftext'] = input['s_selftext'].apply(str.lower)\n",
    "\n",
    "# Remove all punction\n",
    "input['s_selftext']: list[str] = [re.sub('[^a-zA-Z]', ' ', x) for x in input['s_selftext']]\n",
    "\n",
    "#Lemmatize post \n",
    "input['lemmatized_text'] = input.s_selftext.apply(lemmatize_text)\n",
    "\n",
    "X = input['lemmatized_text']\n",
    "Y = input['label']\n",
    "\n",
    "# Stratify the split by the target label so that both sets have the balanced cases of both labels\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.25, random_state=0, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alcoholic post: 6769\n",
      " Non-Alcoholic post: 6630\n"
     ]
    }
   ],
   "source": [
    "print(\"Alcoholic post: {alcoholic}\\n Non-Alcoholic post: {nalcoholic}\".format(alcoholic=len(input[input['label'] == 1]), nalcoholic=len(input[input['label'] == 0])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extracting using TF IDF Vectorizer\n",
    "https://www.analyticsvidhya.com/blog/2021/09/creating-a-movie-reviews-classifier-using-tf-idf-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 10049, n_features: 159796\n"
     ]
    }
   ],
   "source": [
    "#tf idf\n",
    "tf_idf = TfidfVectorizer(ngram_range = (1,3), analyzer='word')\n",
    "#applying tf idf to training data. Learn vocabulary and idf, return document-term matrix.\n",
    "X_train_tf = tf_idf.fit_transform(train_X)\n",
    "#applying tf idf to training data Transform documents to document-term matrix.\n",
    "#Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform).\n",
    "X_train_tf = tf_idf.transform(train_X)\n",
    "\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TFIDF.pkl', 'wb') as file:\n",
    "    pickle.dump(tf_idf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 3350, n_features: 159796\n"
     ]
    }
   ],
   "source": [
    "#transforming test data into tf-idf matrix\n",
    "X_test_tf = tf_idf.transform(test_X)\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test_tf.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE Oversampling (Not neccessary now, since have balanced target label cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, TRAIN counts of label '1': 2245\n",
      "Before OverSampling, TRAIN counts of label '0': 689 \n",
      "\n",
      "Before OverSampling, TEST counts of label '1': 749\n",
      "Before OverSampling, TEST counts of label '0': 230 \n",
      "\n",
      "After OverSampling, the shape of train_X: (4490, 509945)\n",
      "After OverSampling, the shape of train_y: (4490,) \n",
      "\n",
      "After OverSampling, TRAIN counts of label '1': 2245\n",
      "After OverSampling, TRAIN counts of label '0': 2245\n",
      "After OverSampling, TEST counts of label '1': 749\n",
      "After OverSampling, TEST counts of label '0': 230\n"
     ]
    }
   ],
   "source": [
    "print(\"Before OverSampling, TRAIN counts of label '1': {}\".format(sum(train_Y == 1)))\n",
    "print(\"Before OverSampling, TRAIN counts of label '0': {} \\n\".format(sum(train_Y == 0)))\n",
    "\n",
    "print(\"Before OverSampling, TEST counts of label '1': {}\".format(sum(test_Y == 1)))\n",
    "print(\"Before OverSampling, TEST counts of label '0': {} \\n\".format(sum(test_Y == 0)))\n",
    "\n",
    "sm = SMOTE(random_state = 2)\n",
    "X_train_tf, train_Y = sm.fit_resample(X_train_tf, train_Y.ravel())\n",
    "\n",
    "#X_test_tf, test_Y = sm.fit_resample(X_test_tf, test_Y.ravel())\n",
    "\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_tf.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(train_Y.shape))\n",
    "\n",
    "print(\"After OverSampling, TRAIN counts of label '1': {}\".format(sum(train_Y == 1)))\n",
    "print(\"After OverSampling, TRAIN counts of label '0': {}\".format(sum(train_Y == 0)))\n",
    "\n",
    "print(\"After OverSampling, TEST counts of label '1': {}\".format(sum(test_Y == 1)))\n",
    "print(\"After OverSampling, TEST counts of label '0': {}\".format(sum(test_Y == 0)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "Recent Relapse       0.75      0.64      0.69      1658\n",
      "   Not Relapse       0.69      0.80      0.74      1692\n",
      "\n",
      "      accuracy                           0.72      3350\n",
      "     macro avg       0.72      0.72      0.72      3350\n",
      "  weighted avg       0.72      0.72      0.72      3350\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " True Pos \t False Pos\n",
      " False Neg\t True Neg\n",
      "[[1059  599]\n",
      " [ 346 1346]]\n",
      "0.7179104477611941\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train_tf, train_Y)\n",
    "\n",
    "#predicted y\n",
    "y_pred = naive_bayes_classifier.predict(X_test_tf)\n",
    "\n",
    "print(classification_report(test_Y, y_pred, target_names=['Recent Relapse', 'Not Relapse']))\n",
    "\n",
    "print('\\nConfusion matrix:\\n True Pos \\t False Pos\\n False Neg\\t True Neg')\n",
    "print(confusion_matrix(test_Y, y_pred))\n",
    "\n",
    "print(accuracy_score(test_Y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned hyperparameter : (Best params) {'alpha': 1.0}\n",
      "Tuned hyperparameter : (Best score/accuracy) 0.7136037086840561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "C = np.array([50,10,1,0.1,0.01,0.001,0.0001,0])\n",
    "\n",
    "params ={\n",
    "    \"alpha\": C\n",
    "}\n",
    "grid = GridSearchCV(naive_bayes_classifier, params)\n",
    "\n",
    "grid.fit(X_train_tf, train_Y)\n",
    "print(\"Tuned hyperparameter : (Best params)\", grid.best_params_)\n",
    "print(\"Tuned hyperparameter : (Best score/accuracy)\", grid.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "Recent Relapse       0.71      0.69      0.70      1658\n",
      "   Not Relapse       0.70      0.73      0.72      1692\n",
      "\n",
      "      accuracy                           0.71      3350\n",
      "     macro avg       0.71      0.71      0.71      3350\n",
      "  weighted avg       0.71      0.71      0.71      3350\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " True Pos \t False Pos\n",
      " False Neg\t True Neg\n",
      "[[1140  518]\n",
      " [ 456 1236]]\n",
      "0.7092537313432836\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_classifier = LogisticRegression()\n",
    "logistic_regression_classifier.fit(X_train_tf, train_Y)\n",
    "\n",
    "#predicted y\n",
    "y_pred = logistic_regression_classifier.predict(X_test_tf)\n",
    "\n",
    "print(classification_report(test_Y, y_pred, target_names=['Recent Relapse', 'Not Relapse']))\n",
    "\n",
    "print('\\nConfusion matrix:\\n True Pos \\t False Pos\\n False Neg\\t True Neg')\n",
    "print(confusion_matrix(test_Y, y_pred))\n",
    "\n",
    "print(accuracy_score(test_Y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "210 fits failed out of a total of 360.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1216, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\base.py\", line 570, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'C' parameter of LogisticRegression must be a float in the range (0, inf]. Got 0.0 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.70484566 0.70176162        nan        nan        nan 0.67887392\n",
      " 0.70693655 0.70693655 0.70693655        nan        nan        nan\n",
      " 0.70484566 0.70176162        nan        nan        nan 0.55856184\n",
      " 0.7006669  0.7006669  0.70086595        nan        nan        nan\n",
      " 0.70484566 0.70176162        nan        nan        nan 0.4947756\n",
      " 0.54473105 0.54473105 0.54751761        nan        nan        nan\n",
      " 0.70484566 0.70176162        nan        nan        nan 0.4947756\n",
      " 0.5052244  0.5052244  0.5052244         nan        nan        nan\n",
      " 0.70484566 0.70176162        nan        nan        nan 0.4947756\n",
      " 0.5052244  0.5052244  0.5052244         nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned hyperparameter : (Best params) {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Tuned hyperparameter : (Best score/accuracy) 0.7069365467337281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nmodel = LogisticRegression()\\nsolvers = [\\'newton-cg\\', \\'lbfgs\\', \\'liblinear\\']\\npenalty = [\\'l2\\']\\nc_values = [100, 10, 1.0, 0.1, 0.01]\\n# define grid search\\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\\n\\n\\nC = np.array([1,0.1,0.01,0.001,0.0001,0])\\nrand_params = {\\'C\\': C\\n            #,     \\n}\\nseed = 10\\niterations = 100\\n\\n\\nrsearch = RandomizedSearchCV(estimator=logistic_regression_classifier\\n                                , param_distributions=rand_params\\n                                , n_iter=iterations, random_state=seed)\\n\\n\\nrsearch.fit(X_train_tf, train_Y)\\nprint(\"Tuned hyperparameter : (Best params)\", rsearch.best_params_)\\nprint(\"Tuned hyperparameter : (Best score/accuracy)\", rsearch.best_score_)\\nprint(rsearch.best_estimator_.C)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyperparameter Tuning\n",
    "C = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "\n",
    "params ={\n",
    "    \"C\": C,\n",
    "    \"penalty\": [None, 'l1', 'l2', 'elasticnet'],\n",
    "    \"solver\" : ['newton-cg', 'lbfgs', 'liblinear']\n",
    "\n",
    "}\n",
    "grid = GridSearchCV(logistic_regression_classifier, params)\n",
    "\n",
    "grid.fit(X_train_tf, train_Y)\n",
    "print(\"Tuned hyperparameter : (Best params)\", grid.best_params_)\n",
    "print(\"Tuned hyperparameter : (Best score/accuracy)\", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "Recent Relapse       0.71      0.69      0.70      1658\n",
      "   Not Relapse       0.70      0.73      0.72      1692\n",
      "\n",
      "      accuracy                           0.71      3350\n",
      "     macro avg       0.71      0.71      0.71      3350\n",
      "  weighted avg       0.71      0.71      0.71      3350\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " True Pos \t False Pos\n",
      " False Neg\t True Neg\n",
      "[[1140  518]\n",
      " [ 456 1236]]\n",
      "0.7092537313432836\n"
     ]
    }
   ],
   "source": [
    "#After Tuning\n",
    "logistic_regression_classifier = LogisticRegression(C=1.0, penalty= 'l2', solver= 'newton-cg')\n",
    "logistic_regression_classifier.fit(X_train_tf, train_Y)\n",
    "\n",
    "#predicted y\n",
    "y_pred = logistic_regression_classifier.predict(X_test_tf)\n",
    "\n",
    "print(classification_report(test_Y, y_pred, target_names=['Recent Relapse', 'Not Relapse']))\n",
    "\n",
    "print('\\nConfusion matrix:\\n True Pos \\t False Pos\\n False Neg\\t True Neg')\n",
    "print(confusion_matrix(test_Y, y_pred))\n",
    "\n",
    "print(accuracy_score(test_Y, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "Recent Relapse       0.71      0.69      0.70      1658\n",
      "   Not Relapse       0.70      0.73      0.72      1692\n",
      "\n",
      "      accuracy                           0.71      3350\n",
      "     macro avg       0.71      0.71      0.71      3350\n",
      "  weighted avg       0.71      0.71      0.71      3350\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " True Pos \t False Pos\n",
      " False Neg\t True Neg\n",
      "[[1142  516]\n",
      " [ 462 1230]]\n",
      "0.7080597014925373\n"
     ]
    }
   ],
   "source": [
    "SVC_clf = LinearSVC()\n",
    "SVC_clf.fit(X_train_tf, train_Y)\n",
    "\n",
    "#predicted y\n",
    "y_pred = SVC_clf.predict(X_test_tf)\n",
    "\n",
    "print(classification_report(test_Y, y_pred, target_names=['Recent Relapse', 'Not Relapse']))\n",
    "\n",
    "print('\\nConfusion matrix:\\n True Pos \\t False Pos\\n False Neg\\t True Neg')\n",
    "print(confusion_matrix(test_Y, y_pred))\n",
    "\n",
    "print(accuracy_score(test_Y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned hyperparameter : (Best params) {'C': 1.0, 'loss': 'hinge', 'multi_class': 'ovr', 'penalty': 'l2'}\n",
      "Tuned hyperparameter : (Best score/accuracy) 0.7150955030719969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "215 fits failed out of a total of 320.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "140 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 261, in fit\n",
      "    self._validate_params()\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\base.py\", line 570, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of LinearSVC must be a str among {'hinge', 'squared_hinge'}. Got 'square-hinge' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 261, in fit\n",
      "    self._validate_params()\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\base.py\", line 570, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"d:\\Python\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'C' parameter of LinearSVC must be a float in the range (0.0, inf). Got 0.0 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "d:\\Python\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.71111456 0.70882576 0.70882576        nan        nan\n",
      "        nan        nan        nan 0.71151252 0.71151262 0.71151262\n",
      "        nan        nan        nan        nan        nan 0.7150955\n",
      " 0.71449859 0.71449859        nan        nan        nan        nan\n",
      "        nan 0.58772058 0.69071536 0.69071536        nan        nan\n",
      "        nan        nan        nan 0.5052244  0.5052244  0.5052244\n",
      "        nan        nan        nan        nan        nan 0.5052244\n",
      " 0.5052244  0.5052244         nan        nan        nan        nan\n",
      "        nan 0.5052244  0.5052244  0.5052244         nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter Tuning \n",
    "C = np.array([50, 10,1,0.1,0.01,0.001,0.0001,0])\n",
    "\n",
    "params ={\n",
    "    \"C\": C,\n",
    "    \"penalty\" : ['l1','l2'],\n",
    "    \"loss\" : ['hinge', 'square-hinge'],\n",
    "    \"multi_class\" : ['ovr' ,'crammer_singer'],\n",
    "}\n",
    "grid = GridSearchCV(SVC_clf, params)\n",
    "\n",
    "grid.fit(X_train_tf, train_Y)\n",
    "print(\"Tuned hyperparameter : (Best params)\", grid.best_params_)\n",
    "print(\"Tuned hyperparameter : (Best score/accuracy)\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "Recent Relapse       0.73      0.68      0.70      1658\n",
      "   Not Relapse       0.71      0.75      0.73      1692\n",
      "\n",
      "      accuracy                           0.72      3350\n",
      "     macro avg       0.72      0.72      0.72      3350\n",
      "  weighted avg       0.72      0.72      0.72      3350\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " True Pos \t False Pos\n",
      " False Neg\t True Neg\n",
      "[[1132  526]\n",
      " [ 423 1269]]\n",
      "0.7167164179104477\n"
     ]
    }
   ],
   "source": [
    "#After Tuning\n",
    "SVC_clf = LinearSVC(C=1.0, loss= 'hinge', multi_class= 'ovr', penalty= 'l2')\n",
    "SVC_clf.fit(X_train_tf, train_Y)\n",
    "\n",
    "#predicted y\n",
    "y_pred = SVC_clf.predict(X_test_tf)\n",
    "\n",
    "print(classification_report(test_Y, y_pred, target_names=['Recent Relapse', 'Not Relapse']))\n",
    "\n",
    "print('\\nConfusion matrix:\\n True Pos \\t False Pos\\n False Neg\\t True Neg')\n",
    "print(confusion_matrix(test_Y, y_pred))\n",
    "\n",
    "print(accuracy_score(test_Y, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier (Takes 30s, and poor performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "Recent Relapse       0.62      0.53      0.57      1658\n",
      "   Not Relapse       0.60      0.68      0.64      1692\n",
      "\n",
      "      accuracy                           0.61      3350\n",
      "     macro avg       0.61      0.60      0.60      3350\n",
      "  weighted avg       0.61      0.61      0.60      3350\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " True Pos \t False Pos\n",
      " False Neg\t True Neg\n",
      "[[ 872  786]\n",
      " [ 537 1155]]\n",
      "0.6050746268656716\n"
     ]
    }
   ],
   "source": [
    "D_clf = DecisionTreeClassifier()\n",
    "D_clf.fit(X_train_tf, train_Y)\n",
    "\n",
    "#predicted y\n",
    "y_pred = D_clf.predict(X_test_tf)\n",
    "\n",
    "print(classification_report(test_Y, y_pred, target_names=['Recent Relapse', 'Not Relapse']))\n",
    "\n",
    "print('\\nConfusion matrix:\\n True Pos \\t False Pos\\n False Neg\\t True Neg')\n",
    "print(confusion_matrix(test_Y, y_pred))\n",
    "\n",
    "print(accuracy_score(test_Y, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Models and Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NBmodel.pkl', 'wb') as file:\n",
    "    pickle.dump(naive_bayes_classifier, file)\n",
    "\n",
    "with open('LRmodel.pkl', 'wb') as file:\n",
    "    pickle.dump(logistic_regression_classifier, file)\n",
    "\n",
    "with open('SVCmodel.pkl', 'wb') as file:\n",
    "    pickle.dump(SVC_clf, file) #This is the chosen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the feature coefficient df \n",
    "columns : list[str] = tf_idf.get_feature_names_out()\n",
    "\n",
    "LRfeature_coefs = pd.DataFrame(zip(columns, np.transpose(logistic_regression_classifier.coef_)), columns=['features', 'coef'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the feature coefficient df \n",
    "columns : list[str] = tf_idf.get_feature_names_out()\n",
    "\n",
    "SVCfeature_coefs = pd.DataFrame(zip(columns, np.transpose(SVC_clf.coef_)), columns=['features', 'coef'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LRfeature_coefs_df.pkl', 'wb') as file:\n",
    "    pickle.dump(LRfeature_coefs, file)\n",
    "\n",
    "with open('SVCfeature_coefs_df.pkl', 'wb') as file:\n",
    "    pickle.dump(SVCfeature_coefs, file)\n",
    "\n",
    "# NB does not have get coef attribute"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing a test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing a test prediction\n",
    "raw_txt = \"\"\"\n",
    "\n",
    "So, I'm over 2 months sober after a couple of years of daily drinking. On some days staying sober is easy, on others not so much. So today, I'm trying to remind myself about all the things I've rediscovered so far.\n",
    "\n",
    "Milk, damn whole milk in the evening instead of a drink, I forgot how much I like it. \n",
    "Chocolate, which I didn't have a taste for in the years I drank. \n",
    "Food in the evening has it's own flavour again instead of being muddled by the taste of alcohol. \n",
    "Food in the morning is no longer tasteless from my tastebuds being numb from alcohol/hangover. \n",
    "Waking up feeling less terrible (I'm disabled, I'll probably never wake up truly rested, but howly damn not being hungover really helps). \n",
    "Less overthinking &amp; less anxiety (howly shit I thought alcohol made my anxiety calmer, but no damn it made it way way way worse). \n",
    "No furiously checking my texts and social media in the morning to see if I said something rude/whatever whilst drunk. \n",
    "No more waking up in complete self-loathing. \n",
    "No more daily struggle of saying I'll quit whenever I was drunk, but counting if I had enough alcohol (or needed to go out to get more) as soon as I woke up the next day. \n",
    "Poops, they've gotten way better since quiting. \n",
    "Remembering episodes I watched the evening before. \n",
    "No more lying to friends and family (not all my family know, but all that I want to know for now do). \n",
    "Having found people who understand me, that are there for me and help me, and that I can be there for when they need it (this subreddit, the chat, meetings, and so on).\n",
    "\n",
    "\n",
    "Please share your own rediscoveries. It might give me and others some insight (and new things to try).\n",
    "\n",
    "IWNDWYT\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#Process whole raw_txt for overall TFIDF and CoEfs Score\n",
    "raw_txt = remove_nline(raw_txt)\n",
    "raw_txt =raw_txt.lower()\n",
    "\n",
    "#Remove ALL punctions\n",
    "whole_raw_txt = re.sub('[^a-zA-Z]', ' ', raw_txt)\n",
    "whole_raw_txt = whole_raw_txt.split()\n",
    "whole_raw_txt = [lemmatizer.lemmatize(word) for word in whole_raw_txt if not word in set(stopwords)]\n",
    "processed_whole_raw_txt =[' '.join(whole_raw_txt)]\n",
    "\n",
    "processed_whole_raw_txt_input = tf_idf.transform(processed_whole_raw_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess text for analysis of each sentence\n",
    "\n",
    "# Remove most punctuations except !?,.'\n",
    "raw_txt = str(raw_txt).translate(str.maketrans('', '', '\"#$%&\\()*+-/:;<=>@[\\\\]^_`{|}~'))\n",
    "\n",
    "test = tokenize.sent_tokenize(str(raw_txt), language=\"english\")\n",
    "\n",
    "testsample_df = pd.DataFrame(test, columns= ['s_selftext'])\n",
    "\n",
    "testsample_df = testsample_df.explode('s_selftext')\n",
    "\n",
    "testsample_df['s_selftext']: list[str] = [re.sub('[^a-zA-Z]', ' ', x) for x in testsample_df['s_selftext']]\n",
    "\n",
    "testsample_df['lemmatized_text'] = testsample_df.s_selftext.apply(lemmatize_text)\n",
    "\n",
    "#Get subjectivity score\n",
    "testsample_df['subjectivity'] = [TextBlob(str(i)).sentiment.subjectivity for i in testsample_df['lemmatized_text']]\n",
    "XTest = tf_idf.transform(testsample_df['lemmatized_text'].values)\n",
    "YTest = logistic_regression_classifier.predict(XTest)\n",
    "testsample_df['prediction'] = YTest\n",
    "\n",
    "avgPredict = testsample_df[['prediction','subjectivity']].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the test features with highest TF IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "way way  -  0.12365111329339165\n",
      "way better since  -  0.061825556646695826\n",
      "way  -  0.11493200212252096\n",
      "waking  -  0.082655607359226\n",
      "taste  -  0.0884854898282989\n",
      "soon woke next  -  0.061825556646695826\n",
      "soon woke  -  0.061825556646695826\n",
      "share rediscovery  -  0.061825556646695826\n",
      "poop gotten  -  0.061825556646695826\n",
      "poop  -  0.061825556646695826\n",
      "others  -  0.0686687518175353\n",
      "morning  -  0.0677602984276762\n",
      "milk  -  0.10938498703705334\n",
      "le  -  0.10967030907122677\n",
      "instead  -  0.06944424117895334\n",
      "howly  -  0.11838591056038415\n",
      "help  -  0.06398078134716807\n",
      "food  -  0.0752707944369503\n",
      "family  -  0.06472301694462468\n",
      "evening  -  0.1276178034820504\n",
      "drunk  -  0.06479264492769325\n",
      "damn  -  0.13478048707156126\n",
      "daily  -  0.07607761213136943\n",
      "anxiety  -  0.07005784766070075\n",
      "alcohol  -  0.10944027482329738\n"
     ]
    }
   ],
   "source": [
    "#test_input = tf_idf.transform(raw_txt)\n",
    "feature_array = np.array(tf_idf.get_feature_names_out())\n",
    "#tfidf_sorting = np.argsort(test_input.toarray()).flatten()[::-1]\n",
    "tfidf_sorting = np.argsort(processed_whole_raw_txt_input.toarray()).flatten()[::-1]\n",
    "\n",
    "top_n = feature_array[tfidf_sorting][:25]\n",
    "\n",
    "for i in processed_whole_raw_txt_input.nonzero()[1]:\n",
    "    if i in tfidf_sorting[:25]:\n",
    "        print(feature_array[i], ' - ', processed_whole_raw_txt_input[0, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 feature coefficient\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#Getting the feature coefficient df \n",
    "columns : list[str] = tf_idf.get_feature_names_out()\n",
    "\n",
    "feature_coefs = pd.DataFrame(zip(columns, np.transpose(logistic_regression_classifier.coef_)), columns=['features', 'coef'])\n",
    "\n",
    "\n",
    "#words_input = tf_idf.transform(test_processed)\n",
    "feature_array = np.array(tf_idf.get_feature_names_out())\n",
    "tfidf_sorting = np.argsort(processed_whole_raw_txt_input.toarray()).flatten()[::-1]\n",
    "\n",
    "#idx = feature_coef.index[feature_coef['features']]\n",
    "idx = feature_coefs.index.tolist()\n",
    "feature_coefs['idx'] = idx\n",
    "print('Top 10 feature coefficient')\n",
    "target_idx = [processed_whole_raw_txt_input.nonzero()[1]]\n",
    "target_idx :list[str] = np.transpose(target_idx)\n",
    "input_coefs = pd.DataFrame(columns = feature_coefs.columns )\n",
    "for index, row in feature_coefs.iterrows():\n",
    "    if index in target_idx:\n",
    "        input_coefs = input_coefs.append({\n",
    "            'features': row['features'],\n",
    "            'coef': row['coef'],\n",
    "            'idx': row['idx'],\n",
    "            }, ignore_index =True)\n",
    "\n",
    "input_coefs = input_coefs.sort_values(by='coef', ascending=False)\n",
    "\n",
    "if len(input_coefs.index) < 25:\n",
    "    explanatory_words = input_coefs[['features','coef']][:]\n",
    "else:\n",
    "    explanatory_words = input_coefs[['features','coef']][:25]\n",
    "    #input_coefs.loc[:25, ['features','coef']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>drinking</td>\n",
       "      <td>[5.501821492427032]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>drink</td>\n",
       "      <td>[4.333893632482377]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alcohol</td>\n",
       "      <td>[3.9173885306694114]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>drunk</td>\n",
       "      <td>[3.194857280301748]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>day</td>\n",
       "      <td>[3.131389460639436]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>feeling</td>\n",
       "      <td>[2.8371161112566203]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>sober</td>\n",
       "      <td>[2.5572286562185003]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>iwndwyt</td>\n",
       "      <td>[2.239296174586229]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>today</td>\n",
       "      <td>[2.1500200134212712]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>morning</td>\n",
       "      <td>[2.1372677247652825]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>drank</td>\n",
       "      <td>[1.8390500965744785]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>family</td>\n",
       "      <td>[1.6922803382615703]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>month</td>\n",
       "      <td>[1.570756136398917]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>made</td>\n",
       "      <td>[1.5693289158866037]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>hangover</td>\n",
       "      <td>[1.4334214890179318]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>really</td>\n",
       "      <td>[1.4196707617494035]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>evening</td>\n",
       "      <td>[1.2278125104766826]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>want</td>\n",
       "      <td>[1.1536812245829615]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>far</td>\n",
       "      <td>[1.1073787995171043]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>probably</td>\n",
       "      <td>[1.0014732005079057]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>shit</td>\n",
       "      <td>[0.9946770525284215]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>quit</td>\n",
       "      <td>[0.9521957236743945]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>enough</td>\n",
       "      <td>[0.9331260041941193]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>hungover</td>\n",
       "      <td>[0.885757619231592]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>know</td>\n",
       "      <td>[0.7856976060089041]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     features                  coef\n",
       "49   drinking   [5.501821492427032]\n",
       "48      drink   [4.333893632482377]\n",
       "0     alcohol  [3.9173885306694114]\n",
       "51      drunk   [3.194857280301748]\n",
       "43        day   [3.131389460639436]\n",
       "67    feeling  [2.8371161112566203]\n",
       "216     sober  [2.5572286562185003]\n",
       "110   iwndwyt   [2.239296174586229]\n",
       "251     today  [2.1500200134212712]\n",
       "145   morning  [2.1372677247652825]\n",
       "47      drank  [1.8390500965744785]\n",
       "61     family  [1.6922803382615703]\n",
       "142     month   [1.570756136398917]\n",
       "127      made  [1.5693289158866037]\n",
       "94   hangover  [1.4334214890179318]\n",
       "188    really  [1.4196707617494035]\n",
       "60    evening  [1.2278125104766826]\n",
       "269      want  [1.1536812245829615]\n",
       "66        far  [1.1073787995171043]\n",
       "181  probably  [1.0014732005079057]\n",
       "211      shit  [0.9946770525284215]\n",
       "184      quit  [0.9521957236743945]\n",
       "56     enough  [0.9331260041941193]\n",
       "103  hungover   [0.885757619231592]\n",
       "111      know  [0.7856976060089041]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input_coefs.loc[:25 ,['features','coef']]\n",
    "input_coefs[['features','coef']][:25]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\" \\ncanvas = FigureCanvas(fig)\\nimg = BytesIO()\\nfig.savefig(img)\\nimg.seek(0)\\n\\n#return send_file(img, mimetype='image/png)\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(explanatory_words['features'], explanatory_words['coef'], color='blue')\n",
    "#plt.plot(X, Y2, color='red')\n",
    "\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Coefficient')\n",
    "plt.title('Top 10 Highest Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49     drinking\n",
       "48        drink\n",
       "0       alcohol\n",
       "51        drunk\n",
       "43          day\n",
       "67      feeling\n",
       "216       sober\n",
       "110     iwndwyt\n",
       "251       today\n",
       "145     morning\n",
       "47        drank\n",
       "61       family\n",
       "142       month\n",
       "127        made\n",
       "94     hangover\n",
       "188      really\n",
       "60      evening\n",
       "269        want\n",
       "66          far\n",
       "181    probably\n",
       "211        shit\n",
       "184        quit\n",
       "56       enough\n",
       "103    hungover\n",
       "111        know\n",
       "Name: features, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanatory_words['features']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
